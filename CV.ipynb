{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb \n",
    "from xgboost import plot_importance , XGBClassifier\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "\n",
    "from tqdm import notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original = pd.read_csv('./open data/train.csv')\n",
    "test_original = pd.read_csv('./open data/test_x.csv')\n",
    "train = train_original.copy()\n",
    "test = test_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA\n",
    "def fill_married(data):\n",
    "    pdata = data.copy()\n",
    "    pdata.loc[ (pdata.married==0)&(pdata.age_group=='10s'),'married' ] = 1\n",
    "    pdata.loc[ (pdata.married==0)&(pdata.age_group=='20s'),'married' ] = 1\n",
    "    pdata.loc[pdata.married==0,'married'] = 2\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def fill_education(data):\n",
    "    pdata = data.copy()\n",
    "    pdata.loc[(pdata.education==0)&(pdata.age_group=='10s'),'education'] = 2\n",
    "    pdata.loc[pdata.education==0,'education'] = 3\n",
    "\n",
    "    return pdata\n",
    "\n",
    "def fill_engnat(data):\n",
    "    pdata = data.copy()\n",
    "    pdata.loc[pdata.engnat==0,'engnat'] = 1\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def fill_hand(data):\n",
    "    pdata = data.copy()\n",
    "    pdata.loc[pdata.hand==0,'hand'] = 1\n",
    "    \n",
    "    return pdata\n",
    "# feature engineering\n",
    "def Mach_score(data):\n",
    "    pdata = data.copy()\n",
    "    Answers = []\n",
    "    for i in range(20):\n",
    "        Answers.append('Q'+chr(97+i)+'A')\n",
    "    reverse_col = ['QeA','QfA','QkA','QqA','QrA','QaA','QdA','QgA','QiA','QnA']\n",
    "    for col in reverse_col:\n",
    "        pdata[col] = -pdata[col]\n",
    "    pdata['Mach_score'] = pdata[Answers].sum(axis=1)\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def w_score(data):\n",
    "    pdata = data.copy()\n",
    "    wr = []\n",
    "    wf = []\n",
    "    for i in range(1,14):\n",
    "        wr.append(f'wr_{i:02d}')\n",
    "    for i in range(1,4):\n",
    "        wf.append(f'wf_{i:02d}')\n",
    "    \n",
    "    pdata['wr'] = pdata[wr].sum(axis=1)\n",
    "    pdata['wf'] = pdata[wf].sum(axis=1)\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def TIPI(data):\n",
    "    pdata = data.copy()\n",
    "    pdata['tp_score_1'] = pdata['tp01'] - pdata['tp06']\n",
    "    pdata['tp_score_2'] = pdata['tp07'] - pdata['tp02']\n",
    "    pdata['tp_score_3'] = pdata['tp03'] - pdata['tp08']\n",
    "    pdata['tp_score_4'] = pdata['tp09'] - pdata['tp04']\n",
    "    pdata['tp_score_5'] = pdata['tp05'] - pdata['tp10']\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "# drop outlier\n",
    "def drop_outlier(data, datatype):\n",
    "    \n",
    "    assert datatype == 'train' or datatype=='test', 'Wrong data type given'\n",
    "    \n",
    "    pdata = data.copy()\n",
    "    if datatype=='train':\n",
    "        \n",
    "        out_arr = []\n",
    "        out_arr.append( data[data.familysize>=16].index )\n",
    "        out_arr.append( data[data.wr<=3].index )\n",
    "        out_arr.append( data[data.wf>=2].index )\n",
    "\n",
    "        out = []\n",
    "        for outarr in out_arr:\n",
    "            out = np.union1d(out, outarr)\n",
    "\n",
    "        pdata = data.drop(out)\n",
    "    \n",
    "    return pdata\n",
    "# feature banding\n",
    "def age_band(data):\n",
    "    pdata = data.copy()\n",
    "    pdata['age_group'].replace(['10s','20s','30s','40s','50s','60s','+70s'],[1,2,3,4,5,5,5],inplace=True)\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def E_band(data, num_band):\n",
    "    pdata = data.copy()\n",
    "    for i in range(20):\n",
    "        col = 'Q'+chr(i+97)+'E'\n",
    "        pdata[col] = pd.qcut(pdata[col],num_band)\n",
    "        unique = pdata[col].unique()\n",
    "        pdata[col].replace(unique,range(num_band),inplace=True)\n",
    "        \n",
    "    return pdata\n",
    "\n",
    "def family_band(data):\n",
    "    pdata = data.copy()\n",
    "    pdata.loc[pdata.familysize >= 6,'familysize'] = 6\n",
    "    \n",
    "    return pdata\n",
    "# categorical value to numerical value\n",
    "def cat_gender(data):\n",
    "    feature = 'gender'\n",
    "    pdata = data.copy()\n",
    "    pdata[feature].replace(['Male','Female'],[0,1],inplace=True)\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def cat_race(data):\n",
    "    feature = 'race'\n",
    "    pdata = data.copy()\n",
    "    unique = ['White', 'Asian', 'Other', 'Black', 'Native American', 'Arab', 'Indigenous Australian']\n",
    "    pdata[feature].replace(unique,[0,1,2,3,4,5,6],inplace=True)\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def cat_religion(data):\n",
    "    feature = 'religion'\n",
    "    pdata = data.copy()\n",
    "    unique = ['Other', 'Hindu', 'Agnostic', 'Atheist', 'Christian_Other',\n",
    "       'Christian_Catholic', 'Muslim', 'Buddhist', 'Christian_Protestant',\n",
    "       'Jewish', 'Christian_Mormon', 'Sikh']\n",
    "    pdata[feature].replace(unique,[11,10,0,1,2,3,4,5,6,7,8,9],inplace=True)\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def cat_num(data):\n",
    "    pdata = data.copy()\n",
    "    pdata = cat_gender(pdata)\n",
    "    pdata = cat_race(pdata)\n",
    "    pdata = cat_religion(pdata)\n",
    "    \n",
    "    return pdata\n",
    "# drop feature\n",
    "def drop_feature(data, feature_arr):\n",
    "    arr = feature_arr + ['index'] \n",
    "    \n",
    "    \"\"\"for i in range(20):\n",
    "        arr.append('Q'+chr(i+97)+'A')\n",
    "    for i in range(20):\n",
    "        arr.append('Q'+chr(i+97)+'E')\n",
    "    for i in range(1,14):\n",
    "        arr.append(f'wr_{i:02d}')\n",
    "    for i in range(1,4):\n",
    "        arr.append(f'wf_{i:02d}')\n",
    "    for i in range(1,11):\n",
    "        arr.append(f'tp{i:02d}')\"\"\"\n",
    "    \n",
    "\n",
    "    pdata = data.drop(arr,axis=1)\n",
    "    \n",
    "    return pdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, datatype, feature_arr):\n",
    "    \n",
    "    pdata = data.copy()\n",
    "    # fill NA\n",
    "    pdata = fill_married(pdata)\n",
    "    pdata = fill_education(pdata)\n",
    "    pdata = fill_engnat(pdata)\n",
    "    pdata = fill_hand(pdata)\n",
    "    # feature engineering\n",
    "    pdata = Mach_score(pdata)\n",
    "    pdata = w_score(pdata)\n",
    "    pdata = TIPI(pdata)\n",
    "    # drop outlier\n",
    "    pdata = drop_outlier(pdata,datatype)\n",
    "    # feature banding\n",
    "    pdata = age_band(pdata)\n",
    "    pdata = family_band(pdata)\n",
    "    pdata = E_band(pdata,10)\n",
    "    # categorical value to numerical value\n",
    "    pdata = cat_num(pdata)\n",
    "    # drop feature\n",
    "    pdata = drop_feature(pdata, feature_arr)\n",
    "    # unify type of data\n",
    "    pdata = pdata.astype(np.int)\n",
    "    \n",
    "    return pdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = { 'max_depth' : 8,\n",
    "        'n_estimators' : 200,\n",
    "        'learning_rate' : 0.010,\n",
    "         'min_child_weight' : 6,\n",
    "         'colsample_bytree' : 0.8,\n",
    "        'verbosity' : 0,\n",
    "        'objective' : 'binary:logistic',\n",
    "        'booster' : 'gbtree',\n",
    "        'subsample' : 0.8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_arr_1 = ['QbA','QcA','QjA','QhA','QmA','QtA','QlA','QkA','QoA',\n",
    " 'QsA','QrA', 'QeA',  'QnA','QgA', 'QdA'] + \\\n",
    "['QaE','QbE','QcE','QeE','QgE','QmE','QfE','QiE','QoE',\n",
    " 'QlE','QrE','QpE','QnE','QtE','QsE','QkE'] + \\\n",
    "['tp01','tp04','tp05','tp09','tp10','tp02', 'tp08'] +\\\n",
    "['wf_01', 'wf_03'] + \\\n",
    "['wr_06','wr_09', 'wr_11','wr_07', 'wr_12','wr_13'] +\\\n",
    "['tp_score_4','hand']\n",
    "\n",
    "opt_arr_2 = ['QjA','QaE', 'QbA', 'QeE', 'QfE', 'QhA', 'QiE', 'tp09', 'tp_score_4',\n",
    "      'QbE', 'QtA', 'tp01', 'tp_score_2',\n",
    "       'QmA', 'QmE', 'tp04',\n",
    "       'QgE', 'QkA', 'QoE', 'QsA',\n",
    "       'QlE', 'QoE', 'QrE', 'wf_03',\n",
    "       'QoA', 'QlA', 'QsE', 'tp10',\n",
    "       'QpE', 'tp08', 'wf_01',\n",
    "       'QkE', 'QrA', 'wr_05', 'wr_09', 'wr_10', 'wr_11',\n",
    "       'QgA', 'QtE', 'hand', 'tp06', 'QeA', 'wr_06', 'wr_12',\n",
    "       'wr_03', 'wr_07', 'QdA', 'QdE', 'QnE', 'wr_13'\n",
    "      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(param, arr, seed):\n",
    "    TEST_SIZE = 0.25\n",
    "    \n",
    "    if seed==False:\n",
    "        train_data, val_data = train_test_split(train, test_size=TEST_SIZE, \n",
    "                                                shuffle=False)\n",
    "    else:\n",
    "        train_data, val_data = train_test_split(train, test_size=TEST_SIZE, \n",
    "                                                random_state=seed, shuffle=True)\n",
    "        \n",
    "    train_x = preprocess(train_data,'train',arr)\n",
    "    train_y = train_x['voted']\n",
    "    train_x = train_x.drop(['voted'],axis=1)\n",
    "    val_x = preprocess(val_data,'test',arr)\n",
    "    val_y = val_x['voted']\n",
    "    val_x = val_x.drop(['voted'],axis=1)\n",
    "    \n",
    "    start = time.time()\n",
    "    model = XGBClassifier(**param)\n",
    "    model.fit(train_x,train_y,verbose=False)\n",
    "    print(f'Training time : {time.time()-start:.3f}s')\n",
    "    \n",
    "    pred = model.predict_proba(val_x)[:,1]\n",
    "    val_auc = roc_auc_score(val_y, pred)\n",
    "    \n",
    "    print(f'Validation auc : {val_auc:.6f}')\n",
    "    \n",
    "    return val_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 78.404s\n",
      "Validation auc : 0.768930\n"
     ]
    }
   ],
   "source": [
    "auc1 = cross_validation(opt_gpu, [], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 79.504s\n",
      "Validation auc : 0.768930\n"
     ]
    }
   ],
   "source": [
    "auc1 = cross_validation(test_gpu, [], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 109.502s\n",
      "Validation auc : 0.768312\n"
     ]
    }
   ],
   "source": [
    "auc1 = cross_validation(test_gpu1, [], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 54.355s\n",
      "Validation auc : 0.768780\n"
     ]
    }
   ],
   "source": [
    "auc1 = cross_validation(past_gpu, [], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 35.910s\n",
      "Validation auc : 0.769278\n"
     ]
    }
   ],
   "source": [
    "auc1 = cross_validation(past_gpu1, [], 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFOLD CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cv(param, arr, NFOLD):\n",
    "    \n",
    "    train_copy = train.copy()\n",
    "    \n",
    "    kfold = KFold(n_splits = NFOLD, shuffle=True, random_state=42\n",
    "                 )\n",
    "    model_arr = []\n",
    "    auc_arr = []\n",
    "    \n",
    "    for i, (train_index, val_index) in notebook.tqdm(enumerate(kfold.split(train_copy)), total=kfold.get_n_splits()):\n",
    "        # if i in FOLD:\n",
    "        train_x, val_x = train_copy.iloc[train_index,:], train_copy.iloc[val_index,:]\n",
    "\n",
    "        train_x = preprocess(train_x, 'train', arr)\n",
    "        train_y = train_x['voted']\n",
    "        train_x = train_x.drop(['voted'],axis=1)\n",
    "\n",
    "        val_x = preprocess(val_x, 'test', arr)\n",
    "        val_y = val_x['voted']\n",
    "        val_x = val_x.drop(['voted'],axis=1)\n",
    "\n",
    "        start = time.time()\n",
    "        model = XGBClassifier(**param)\n",
    "        model.fit(train_x,train_y,verbose=False)\n",
    "        print(f'Training time : {time.time()-start:.2f}s')\n",
    "        model_arr.append(model)\n",
    "\n",
    "        pred = model.predict_proba(val_x)[:,1]\n",
    "        val_auc = roc_auc_score(val_y, pred)\n",
    "        auc_arr.append(val_auc)\n",
    "        print(f'{val_auc:.6f}')\n",
    "        \n",
    "    \n",
    "    print(f'{sum(auc_arr)/len(auc_arr):.6f}')\n",
    "    \n",
    "    return model_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gpu = { 'max_depth' : 10,\n",
    "        'n_estimators' : 200,\n",
    "        'learning_rate' : 0.010,\n",
    "         'min_child_weight' : 6,\n",
    "         'colsample_bytree' : 0.8,\n",
    "        'verbosity' : 0,\n",
    "        'objective' : 'binary:logistic',\n",
    "        'booster' : 'gbtree',\n",
    "        'subsample' : 0.8,\n",
    "        'tree_method' : 'gpu_hist'\n",
    "        # ,'predictor' : 'gpu_predictor'\n",
    "       }\n",
    "past_gpu = { 'max_depth' : 9,\n",
    "        'n_estimators' : 200,\n",
    "        'learning_rate' : 0.010,\n",
    "         'min_child_weight' : 6,\n",
    "         'colsample_bytree' : 0.8,\n",
    "        'verbosity' : 0,\n",
    "        'objective' : 'binary:logistic',\n",
    "        'booster' : 'gbtree',\n",
    "        'subsample' : 0.8,\n",
    "        'tree_method' : 'gpu_hist'\n",
    "        # ,'predictor' : 'gpu_predictor'\n",
    "       }\n",
    "past_gpu1 = { 'max_depth' : 8,\n",
    "        'n_estimators' : 200,\n",
    "        'learning_rate' : 0.010,\n",
    "         'min_child_weight' : 6,\n",
    "         'colsample_bytree' : 0.8,\n",
    "        'verbosity' : 0,\n",
    "        'objective' : 'binary:logistic',\n",
    "        'booster' : 'gbtree',\n",
    "        'subsample' : 0.8,\n",
    "        'tree_method' : 'gpu_hist'\n",
    "        # ,'predictor' : 'gpu_predictor'\n",
    "       }\n",
    "test_gpu = { 'max_depth' : 10,\n",
    "        'n_estimators' : 200,\n",
    "        'learning_rate' : 0.010,\n",
    "         'min_child_weight' : 6,\n",
    "         'colsample_bytree' : 0.8,\n",
    "        'verbosity' : 0,\n",
    "        'objective' : 'binary:logistic',\n",
    "        'booster' : 'gbtree',\n",
    "        'subsample' : 0.8,\n",
    "        'tree_method' : 'gpu_hist'\n",
    "        # ,'predictor' : 'gpu_predictor'\n",
    "       }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d34586049549c981092e6e4e285483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 79.51s\n",
      "0.769642\n",
      "Training time : 80.24s\n",
      "0.770747\n",
      "Training time : 81.24s\n",
      "0.758637\n",
      "Training time : 84.08s\n",
      "0.765341\n",
      "\n",
      "0.766092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,\n",
       "               importance_type='gain', interaction_constraints='',\n",
       "               learning_rate=0.01, max_delta_step=0, max_depth=10,\n",
       "               min_child_weight=6, missing=nan,\n",
       "               monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
       "               n_estimators=200, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n",
       "               tree_method='gpu_hist', validate_parameters=1, verbosity=0),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,\n",
       "               importance_type='gain', interaction_constraints='',\n",
       "               learning_rate=0.01, max_delta_step=0, max_depth=10,\n",
       "               min_child_weight=6, missing=nan,\n",
       "               monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
       "               n_estimators=200, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n",
       "               tree_method='gpu_hist', validate_parameters=1, verbosity=0),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,\n",
       "               importance_type='gain', interaction_constraints='',\n",
       "               learning_rate=0.01, max_delta_step=0, max_depth=10,\n",
       "               min_child_weight=6, missing=nan,\n",
       "               monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
       "               n_estimators=200, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n",
       "               tree_method='gpu_hist', validate_parameters=1, verbosity=0),\n",
       " XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=0,\n",
       "               importance_type='gain', interaction_constraints='',\n",
       "               learning_rate=0.01, max_delta_step=0, max_depth=10,\n",
       "               min_child_weight=6, missing=nan,\n",
       "               monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
       "               n_estimators=200, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n",
       "               tree_method='gpu_hist', validate_parameters=1, verbosity=0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modellist = kfold_cv(opt_gpu, [], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd75100a07a4642a78b8e8a8e63e104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time : 36.37s\n",
      "0.770386\n",
      "Training time : 36.23s\n",
      "0.771505\n",
      "Training time : 36.50s\n",
      "0.759038\n",
      "Training time : 36.29s\n",
      "0.765727\n",
      "\n",
      "0.766664\n"
     ]
    }
   ],
   "source": [
    "modellist = kfold_cv(past_gpu1, [], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
