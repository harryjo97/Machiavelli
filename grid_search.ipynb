{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb \n",
    "from xgboost import plot_importance , XGBClassifier\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original = pd.read_csv('./open data/train.csv')\n",
    "test_original = pd.read_csv('./open data/test_x.csv')\n",
    "train = train_original.copy()\n",
    "test = test_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA\n",
    "def fill_married(data):\n",
    "    pdata = data.copy()\n",
    "    pdata.loc[ (pdata.married==0)&(pdata.age_group=='10s'),'married' ] = 1\n",
    "    pdata.loc[ (pdata.married==0)&(pdata.age_group=='20s'),'married' ] = 1\n",
    "    pdata.loc[pdata.married==0,'married'] = 2\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def fill_education(data):\n",
    "    pdata = data.copy()\n",
    "    pdata.loc[(pdata.education==0)&(pdata.age_group=='10s'),'education'] = 2\n",
    "    pdata.loc[pdata.education==0,'education'] = 3\n",
    "\n",
    "    return pdata\n",
    "\n",
    "def fill_engnat(data):\n",
    "    pdata = data.copy()\n",
    "    pdata.loc[pdata.engnat==0,'engnat'] = 1\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def fill_hand(data):\n",
    "    pdata = data.copy()\n",
    "    pdata.loc[pdata.hand==0,'hand'] = 1\n",
    "    \n",
    "    return pdata\n",
    "# feature engineering\n",
    "def Mach_score(data):\n",
    "    pdata = data.copy()\n",
    "    Answers = []\n",
    "    for i in range(20):\n",
    "        Answers.append('Q'+chr(97+i)+'A')\n",
    "    reverse_col = ['QeA','QfA','QkA','QqA','QrA','QaA','QdA','QgA','QiA','QnA']\n",
    "    for col in reverse_col:\n",
    "        pdata[col] = -pdata[col]\n",
    "    pdata['Mach_score'] = pdata[Answers].sum(axis=1)\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def w_score(data):\n",
    "    pdata = data.copy()\n",
    "    wr = []\n",
    "    wf = []\n",
    "    for i in range(1,14):\n",
    "        wr.append(f'wr_{i:02d}')\n",
    "    for i in range(1,4):\n",
    "        wf.append(f'wf_{i:02d}')\n",
    "    \n",
    "    pdata['wr'] = pdata[wr].sum(axis=1)\n",
    "    pdata['wf'] = pdata[wf].sum(axis=1)\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def TIPI(data):\n",
    "    pdata = data.copy()\n",
    "    pdata['tp_score_1'] = pdata['tp01'] - pdata['tp06']\n",
    "    pdata['tp_score_2'] = pdata['tp07'] - pdata['tp02']\n",
    "    pdata['tp_score_3'] = pdata['tp03'] - pdata['tp08']\n",
    "    pdata['tp_score_4'] = pdata['tp09'] - pdata['tp04']\n",
    "    pdata['tp_score_5'] = pdata['tp05'] - pdata['tp10']\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "# drop outlier\n",
    "def drop_outlier(data, datatype):\n",
    "    \n",
    "    assert datatype == 'train' or datatype=='test', 'Wrong data type given'\n",
    "    \n",
    "    pdata = data.copy()\n",
    "    if datatype=='train':\n",
    "        \n",
    "        out_arr = []\n",
    "        out_arr.append( data[data.familysize>=16].index )\n",
    "        out_arr.append( data[data.wr<=3].index )\n",
    "        out_arr.append( data[data.wf>=2].index )\n",
    "\n",
    "        out = []\n",
    "        for outarr in out_arr:\n",
    "            out = np.union1d(out, outarr)\n",
    "\n",
    "        pdata = data.drop(out)\n",
    "    \n",
    "    return pdata\n",
    "# feature banding\n",
    "def age_band(data):\n",
    "    pdata = data.copy()\n",
    "    pdata['age_group'].replace(['10s','20s','30s','40s','50s','60s','+70s'],[1,2,3,4,5,5,5],inplace=True)\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def E_band(data, num_band):\n",
    "    pdata = data.copy()\n",
    "    for i in range(20):\n",
    "        col = 'Q'+chr(i+97)+'E'\n",
    "        pdata[col] = pd.qcut(pdata[col],num_band)\n",
    "        unique = pdata[col].unique()\n",
    "        pdata[col].replace(unique,range(num_band),inplace=True)\n",
    "        \n",
    "    return pdata\n",
    "\n",
    "def family_band(data):\n",
    "    pdata = data.copy()\n",
    "    pdata.loc[pdata.familysize >= 6,'familysize'] = 6\n",
    "    \n",
    "    return pdata\n",
    "# categorical value to numerical value\n",
    "def cat_gender(data):\n",
    "    feature = 'gender'\n",
    "    pdata = data.copy()\n",
    "    pdata[feature].replace(['Male','Female'],[0,1],inplace=True)\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def cat_race(data):\n",
    "    feature = 'race'\n",
    "    pdata = data.copy()\n",
    "    unique = ['White', 'Asian', 'Other', 'Black', 'Native American', 'Arab', 'Indigenous Australian']\n",
    "    pdata[feature].replace(unique,[0,1,2,3,4,5,6],inplace=True)\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def cat_religion(data):\n",
    "    feature = 'religion'\n",
    "    pdata = data.copy()\n",
    "    unique = ['Other', 'Hindu', 'Agnostic', 'Atheist', 'Christian_Other',\n",
    "       'Christian_Catholic', 'Muslim', 'Buddhist', 'Christian_Protestant',\n",
    "       'Jewish', 'Christian_Mormon', 'Sikh']\n",
    "    pdata[feature].replace(unique,[11,10,0,1,2,3,4,5,6,7,8,9],inplace=True)\n",
    "    \n",
    "    return pdata\n",
    "\n",
    "def cat_num(data):\n",
    "    pdata = data.copy()\n",
    "    pdata = cat_gender(pdata)\n",
    "    pdata = cat_race(pdata)\n",
    "    pdata = cat_religion(pdata)\n",
    "    \n",
    "    return pdata\n",
    "# drop feature\n",
    "def drop_feature(data, feature_arr):\n",
    "    arr = feature_arr + ['index'] \n",
    "    \n",
    "    \"\"\"for i in range(20):\n",
    "        arr.append('Q'+chr(i+97)+'A')\n",
    "    for i in range(20):\n",
    "        arr.append('Q'+chr(i+97)+'E')\n",
    "    for i in range(1,14):\n",
    "        arr.append(f'wr_{i:02d}')\n",
    "    for i in range(1,4):\n",
    "        arr.append(f'wf_{i:02d}')\n",
    "    for i in range(1,11):\n",
    "        arr.append(f'tp{i:02d}')\"\"\"\n",
    "    \n",
    "\n",
    "    pdata = data.drop(arr,axis=1)\n",
    "    \n",
    "    return pdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, datatype, feature_arr):\n",
    "    \n",
    "    pdata = data.copy()\n",
    "    # fill NA\n",
    "    pdata = fill_married(pdata)\n",
    "    pdata = fill_education(pdata)\n",
    "    pdata = fill_engnat(pdata)\n",
    "    pdata = fill_hand(pdata)\n",
    "    # feature engineering\n",
    "    pdata = Mach_score(pdata)\n",
    "    pdata = w_score(pdata)\n",
    "    pdata = TIPI(pdata)\n",
    "    # drop outlier\n",
    "    pdata = drop_outlier(pdata,datatype)\n",
    "    # feature banding\n",
    "    pdata = age_band(pdata)\n",
    "    pdata = family_band(pdata)\n",
    "    pdata = E_band(pdata,10)\n",
    "    # categorical value to numerical value\n",
    "    pdata = cat_num(pdata)\n",
    "    # drop feature\n",
    "    pdata = drop_feature(pdata, feature_arr)\n",
    "    # unify type of data\n",
    "    pdata = pdata.astype(np.int)\n",
    "    \n",
    "    return pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_auc(model_arr, data, label):\n",
    "    score = np.zeros((data.shape[0],2))\n",
    "    num_model = len(model_arr)\n",
    "    for i in range(num_model):\n",
    "        score += model_arr[i].predict_proba(data)\n",
    "    pred = np.divide(score,num_model)[:,1]\n",
    "    \n",
    "    return roc_auc_score(label, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train,test_size=0.14998, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(max_depth_arr, n_estimators_arr, learning_rate_arr, min_child_arr,col_sample_arr):\n",
    "\n",
    "    opt_auc = 0\n",
    "    opt_para = {}\n",
    "    \n",
    "    for max_depth in max_depth_arr:\n",
    "        for n_estimators in n_estimators_arr:\n",
    "            for learning_rate in learning_rate_arr:\n",
    "                for min_child in min_child_arr:\n",
    "                    for col_sample in col_sample_arr:\n",
    "                        print(f'{max_depth}_{n_estimators}_{learning_rate:.3f}_{min_child:02d}_{col_sample:.2f}', end=' ')\n",
    "                        param = { 'max_depth' : max_depth,\n",
    "                                'n_estimators' : n_estimators,\n",
    "                                'learning_rate' : learning_rate,\n",
    "                                 'min_child_weight' : min_child,\n",
    "                                 'colsample_bytree' : col_sample,\n",
    "                                'verbosity' : 0,\n",
    "                                'objective' : 'binary:logistic',\n",
    "                                'booster' : 'gbtree',\n",
    "                                'subsample' : 0.8}\n",
    "\n",
    "                        model = XGBClassifier(**param)\n",
    "                        model.fit(train_x, train_y, verbose=False)\n",
    "                        auc = train_auc([model], val_x, val_y) \n",
    "                        print('\\033[34m' + f'{auc:.6f}' + '\\033[0m', end=' ')\n",
    "\n",
    "                        if (auc>opt_auc):\n",
    "                            opt_auc = auc\n",
    "                            opt_para = param\n",
    "                            print('\\033[31m' + f'{max_depth}_{n_estimators}_{learning_rate:.3f}_{min_child:02d}_{col_sample:.2f}' + '\\033[0m')\n",
    "                        else:\n",
    "                            print()\n",
    "    \n",
    "    print('-'*30)\n",
    "    print(f'{opt_para} = ' + '\\033[34m' + f'{opt_auc:.6f}' + '\\033[0m')\n",
    "    \n",
    "    return opt_auc, opt_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8_200_0.010_01_0.75 \u001b[34m0.768783\u001b[0m \u001b[31m8_200_0.010_01_0.75\u001b[0m\n",
      "8_200_0.010_01_0.80 \u001b[34m0.768233\u001b[0m \n",
      "8_200_0.010_01_0.85 \u001b[34m0.768189\u001b[0m \n",
      "8_200_0.010_02_0.75 \u001b[34m0.768878\u001b[0m \u001b[31m8_200_0.010_02_0.75\u001b[0m\n",
      "8_200_0.010_02_0.80 \u001b[34m0.768168\u001b[0m \n",
      "8_200_0.010_02_0.85 \u001b[34m0.768210\u001b[0m \n",
      "8_200_0.010_03_0.75 \u001b[34m0.768450\u001b[0m \n",
      "8_200_0.010_03_0.80 \u001b[34m0.767992\u001b[0m \n",
      "8_200_0.010_03_0.85 \u001b[34m0.768350\u001b[0m \n",
      "8_200_0.010_04_0.75 \u001b[34m0.768546\u001b[0m \n",
      "8_200_0.010_04_0.80 \u001b[34m0.767918\u001b[0m \n",
      "8_200_0.010_04_0.85 \u001b[34m0.767939\u001b[0m \n",
      "8_200_0.010_05_0.75 \u001b[34m0.768700\u001b[0m \n",
      "8_200_0.010_05_0.80 \u001b[34m0.768138\u001b[0m \n",
      "8_200_0.010_05_0.85 \u001b[34m0.768314\u001b[0m \n",
      "8_200_0.010_06_0.75 \u001b[34m0.768796\u001b[0m \n",
      "8_200_0.010_06_0.80 \u001b[34m0.768319\u001b[0m \n",
      "8_200_0.010_06_0.85 \u001b[34m0.768236\u001b[0m \n",
      "8_200_0.010_07_0.75 \u001b[34m0.768590\u001b[0m \n",
      "8_200_0.010_07_0.80 \u001b[34m0.767794\u001b[0m \n",
      "8_200_0.010_07_0.85 \u001b[34m0.768094\u001b[0m \n",
      "8_200_0.010_08_0.75 \u001b[34m0.768623\u001b[0m \n",
      "8_200_0.010_08_0.80 \u001b[34m0.767572\u001b[0m \n",
      "8_200_0.010_08_0.85 \u001b[34m0.767744\u001b[0m \n",
      "8_200_0.010_09_0.75 \u001b[34m0.768651\u001b[0m \n",
      "8_200_0.010_09_0.80 \u001b[34m0.767331\u001b[0m \n",
      "8_200_0.010_09_0.85 \u001b[34m0.767714\u001b[0m \n",
      "8_200_0.010_10_0.75 \u001b[34m0.768441\u001b[0m \n",
      "8_200_0.010_10_0.80 \u001b[34m0.767556\u001b[0m \n",
      "8_200_0.010_10_0.85 \u001b[34m0.767526\u001b[0m \n",
      "8_250_0.010_01_0.75 \u001b[34m0.768950\u001b[0m \u001b[31m8_250_0.010_01_0.75\u001b[0m\n",
      "8_250_0.010_01_0.80 \u001b[34m0.768411\u001b[0m \n",
      "8_250_0.010_01_0.85 \u001b[34m0.768311\u001b[0m \n",
      "8_250_0.010_02_0.75 \u001b[34m0.768967\u001b[0m \u001b[31m8_250_0.010_02_0.75\u001b[0m\n",
      "8_250_0.010_02_0.80 \u001b[34m0.768169\u001b[0m \n",
      "8_250_0.010_02_0.85 \u001b[34m0.768130\u001b[0m \n",
      "8_250_0.010_03_0.75 \u001b[34m0.768599\u001b[0m \n",
      "8_250_0.010_03_0.80 \u001b[34m0.767911\u001b[0m \n",
      "8_250_0.010_03_0.85 \u001b[34m0.768353\u001b[0m \n",
      "8_250_0.010_04_0.75 \u001b[34m0.768776\u001b[0m \n",
      "8_250_0.010_04_0.80 \u001b[34m0.767934\u001b[0m \n",
      "8_250_0.010_04_0.85 \u001b[34m0.767950\u001b[0m \n",
      "8_250_0.010_05_0.75 \u001b[34m0.768813\u001b[0m \n",
      "8_250_0.010_05_0.80 \u001b[34m0.768174\u001b[0m \n",
      "8_250_0.010_05_0.85 \u001b[34m0.768247\u001b[0m \n",
      "8_250_0.010_06_0.75 \u001b[34m0.768982\u001b[0m \u001b[31m8_250_0.010_06_0.75\u001b[0m\n",
      "8_250_0.010_06_0.80 \u001b[34m0.768387\u001b[0m \n",
      "8_250_0.010_06_0.85 \u001b[34m0.768149\u001b[0m \n",
      "8_250_0.010_07_0.75 \u001b[34m0.768731\u001b[0m \n",
      "8_250_0.010_07_0.80 \u001b[34m0.767833\u001b[0m \n",
      "8_250_0.010_07_0.85 \u001b[34m0.768000\u001b[0m \n",
      "8_250_0.010_08_0.75 \u001b[34m0.768815\u001b[0m \n",
      "8_250_0.010_08_0.80 \u001b[34m0.767711\u001b[0m \n",
      "8_250_0.010_08_0.85 \u001b[34m0.767706\u001b[0m \n",
      "8_250_0.010_09_0.75 \u001b[34m0.768851\u001b[0m \n",
      "8_250_0.010_09_0.80 \u001b[34m0.767520\u001b[0m \n",
      "8_250_0.010_09_0.85 \u001b[34m0.767720\u001b[0m \n",
      "8_250_0.010_10_0.75 \u001b[34m0.768494\u001b[0m \n",
      "8_250_0.010_10_0.80 \u001b[34m0.767655\u001b[0m \n",
      "8_250_0.010_10_0.85 \u001b[34m0.767434\u001b[0m \n",
      "8_300_0.010_01_0.75 \u001b[34m0.768879\u001b[0m \n",
      "8_300_0.010_01_0.80 \u001b[34m0.768332\u001b[0m \n",
      "8_300_0.010_01_0.85 \u001b[34m0.768339\u001b[0m \n",
      "8_300_0.010_02_0.75 \u001b[34m0.768881\u001b[0m \n",
      "8_300_0.010_02_0.80 \u001b[34m0.768105\u001b[0m \n",
      "8_300_0.010_02_0.85 \u001b[34m0.768146\u001b[0m \n",
      "8_300_0.010_03_0.75 \u001b[34m0.768669\u001b[0m \n",
      "8_300_0.010_03_0.80 \u001b[34m0.767699\u001b[0m \n",
      "8_300_0.010_03_0.85 \u001b[34m0.768174\u001b[0m \n",
      "8_300_0.010_04_0.75 \u001b[34m0.768829\u001b[0m \n",
      "8_300_0.010_04_0.80 \u001b[34m0.767910\u001b[0m \n",
      "8_300_0.010_04_0.85 \u001b[34m0.767886\u001b[0m \n",
      "8_300_0.010_05_0.75 \u001b[34m0.768827\u001b[0m \n",
      "8_300_0.010_05_0.80 \u001b[34m0.768048\u001b[0m \n",
      "8_300_0.010_05_0.85 \u001b[34m0.768256\u001b[0m \n",
      "8_300_0.010_06_0.75 \u001b[34m0.768985\u001b[0m \u001b[31m8_300_0.010_06_0.75\u001b[0m\n",
      "8_300_0.010_06_0.80 \u001b[34m0.768209\u001b[0m \n",
      "8_300_0.010_06_0.85 \u001b[34m0.768045\u001b[0m \n",
      "8_300_0.010_07_0.75 \u001b[34m0.768722\u001b[0m \n",
      "8_300_0.010_07_0.80 \u001b[34m0.767720\u001b[0m \n",
      "8_300_0.010_07_0.85 \u001b[34m0.767971\u001b[0m \n",
      "8_300_0.010_08_0.75 \u001b[34m0.768839\u001b[0m \n",
      "8_300_0.010_08_0.80 \u001b[34m0.767642\u001b[0m \n",
      "8_300_0.010_08_0.85 \u001b[34m0.767669\u001b[0m \n",
      "8_300_0.010_09_0.75 \u001b[34m0.769046\u001b[0m \u001b[31m8_300_0.010_09_0.75\u001b[0m\n",
      "8_300_0.010_09_0.80 \u001b[34m0.767465\u001b[0m \n",
      "8_300_0.010_09_0.85 \u001b[34m0.767756\u001b[0m \n",
      "8_300_0.010_10_0.75 \u001b[34m0.768704\u001b[0m \n",
      "8_300_0.010_10_0.80 \u001b[34m0.767720\u001b[0m \n",
      "8_300_0.010_10_0.85 \u001b[34m0.767322\u001b[0m \n",
      "9_200_0.010_01_0.75 \u001b[34m0.767771\u001b[0m \n",
      "9_200_0.010_01_0.80 \u001b[34m0.767449\u001b[0m \n",
      "9_200_0.010_01_0.85 \u001b[34m0.767078\u001b[0m \n",
      "9_200_0.010_02_0.75 \u001b[34m0.767774\u001b[0m \n",
      "9_200_0.010_02_0.80 \u001b[34m0.767547\u001b[0m \n",
      "9_200_0.010_02_0.85 \u001b[34m0.766884\u001b[0m \n",
      "9_200_0.010_03_0.75 \u001b[34m0.767443\u001b[0m \n",
      "9_200_0.010_03_0.80 \u001b[34m0.767234\u001b[0m \n",
      "9_200_0.010_03_0.85 \u001b[34m0.767446\u001b[0m \n",
      "9_200_0.010_04_0.75 \u001b[34m0.768046\u001b[0m \n",
      "9_200_0.010_04_0.80 \u001b[34m0.767067\u001b[0m \n",
      "9_200_0.010_04_0.85 \u001b[34m0.767301\u001b[0m \n",
      "9_200_0.010_05_0.75 \u001b[34m0.767717\u001b[0m \n",
      "9_200_0.010_05_0.80 \u001b[34m0.767001\u001b[0m \n",
      "9_200_0.010_05_0.85 \u001b[34m0.767182\u001b[0m \n",
      "9_200_0.010_06_0.75 \u001b[34m0.767988\u001b[0m \n",
      "9_200_0.010_06_0.80 \u001b[34m0.767615\u001b[0m \n",
      "9_200_0.010_06_0.85 \u001b[34m0.767543\u001b[0m \n",
      "9_200_0.010_07_0.75 \u001b[34m0.768026\u001b[0m \n",
      "9_200_0.010_07_0.80 \u001b[34m0.767648\u001b[0m \n",
      "9_200_0.010_07_0.85 \u001b[34m0.767938\u001b[0m \n",
      "9_200_0.010_08_0.75 \u001b[34m0.767930\u001b[0m \n",
      "9_200_0.010_08_0.80 \u001b[34m0.767625\u001b[0m \n",
      "9_200_0.010_08_0.85 \u001b[34m0.767374\u001b[0m \n",
      "9_200_0.010_09_0.75 \u001b[34m0.767903\u001b[0m \n",
      "9_200_0.010_09_0.80 \u001b[34m0.767580\u001b[0m \n",
      "9_200_0.010_09_0.85 \u001b[34m0.767262\u001b[0m \n",
      "9_200_0.010_10_0.75 \u001b[34m0.767812\u001b[0m \n",
      "9_200_0.010_10_0.80 \u001b[34m0.767572\u001b[0m \n",
      "9_200_0.010_10_0.85 \u001b[34m0.767382\u001b[0m \n",
      "9_250_0.010_01_0.75 \u001b[34m0.768116\u001b[0m \n",
      "9_250_0.010_01_0.80 \u001b[34m0.767582\u001b[0m \n",
      "9_250_0.010_01_0.85 \u001b[34m0.766728\u001b[0m \n",
      "9_250_0.010_02_0.75 \u001b[34m0.767806\u001b[0m \n",
      "9_250_0.010_02_0.80 \u001b[34m0.767514\u001b[0m \n",
      "9_250_0.010_02_0.85 \u001b[34m0.766369\u001b[0m \n",
      "9_250_0.010_03_0.75 \u001b[34m0.767293\u001b[0m \n",
      "9_250_0.010_03_0.80 \u001b[34m0.767360\u001b[0m \n",
      "9_250_0.010_03_0.85 \u001b[34m0.767030\u001b[0m \n",
      "9_250_0.010_04_0.75 \u001b[34m0.768093\u001b[0m \n",
      "9_250_0.010_04_0.80 \u001b[34m0.767026\u001b[0m \n",
      "9_250_0.010_04_0.85 \u001b[34m0.766943\u001b[0m \n",
      "9_250_0.010_05_0.75 \u001b[34m0.767788\u001b[0m \n",
      "9_250_0.010_05_0.80 \u001b[34m0.767102\u001b[0m \n",
      "9_250_0.010_05_0.85 \u001b[34m0.766882\u001b[0m \n",
      "9_250_0.010_06_0.75 \u001b[34m0.768011\u001b[0m \n",
      "9_250_0.010_06_0.80 \u001b[34m0.767732\u001b[0m \n",
      "9_250_0.010_06_0.85 \u001b[34m0.767243\u001b[0m \n",
      "9_250_0.010_07_0.75 \u001b[34m0.768043\u001b[0m \n",
      "9_250_0.010_07_0.80 \u001b[34m0.767760\u001b[0m \n",
      "9_250_0.010_07_0.85 \u001b[34m0.767621\u001b[0m \n",
      "9_250_0.010_08_0.75 \u001b[34m0.768015\u001b[0m \n",
      "9_250_0.010_08_0.80 \u001b[34m0.767856\u001b[0m \n",
      "9_250_0.010_08_0.85 \u001b[34m0.767084\u001b[0m \n",
      "9_250_0.010_09_0.75 \u001b[34m0.768002\u001b[0m \n",
      "9_250_0.010_09_0.80 \u001b[34m0.767731\u001b[0m \n",
      "9_250_0.010_09_0.85 \u001b[34m0.766986\u001b[0m \n",
      "9_250_0.010_10_0.75 \u001b[34m0.767795\u001b[0m \n",
      "9_250_0.010_10_0.80 \u001b[34m0.767601\u001b[0m \n",
      "9_250_0.010_10_0.85 \u001b[34m0.767191\u001b[0m \n",
      "9_300_0.010_01_0.75 \u001b[34m0.767855\u001b[0m \n",
      "9_300_0.010_01_0.80 \u001b[34m0.767607\u001b[0m \n",
      "9_300_0.010_01_0.85 \u001b[34m0.766600\u001b[0m \n",
      "9_300_0.010_02_0.75 \u001b[34m0.767362\u001b[0m \n",
      "9_300_0.010_02_0.80 \u001b[34m0.767595\u001b[0m \n",
      "9_300_0.010_02_0.85 \u001b[34m0.766235\u001b[0m \n",
      "9_300_0.010_03_0.75 \u001b[34m0.767218\u001b[0m \n",
      "9_300_0.010_03_0.80 \u001b[34m0.767300\u001b[0m \n",
      "9_300_0.010_03_0.85 \u001b[34m0.766829\u001b[0m \n",
      "9_300_0.010_04_0.75 \u001b[34m0.767861\u001b[0m \n",
      "9_300_0.010_04_0.80 \u001b[34m0.767086\u001b[0m \n",
      "9_300_0.010_04_0.85 \u001b[34m0.766915\u001b[0m \n",
      "9_300_0.010_05_0.75 \u001b[34m0.767708\u001b[0m \n",
      "9_300_0.010_05_0.80 \u001b[34m0.767056\u001b[0m \n",
      "9_300_0.010_05_0.85 \u001b[34m0.766824\u001b[0m \n",
      "9_300_0.010_06_0.75 \u001b[34m0.768061\u001b[0m \n",
      "9_300_0.010_06_0.80 \u001b[34m0.767525\u001b[0m \n",
      "9_300_0.010_06_0.85 \u001b[34m0.767117\u001b[0m \n",
      "9_300_0.010_07_0.75 \u001b[34m0.768031\u001b[0m \n",
      "9_300_0.010_07_0.80 \u001b[34m0.767620\u001b[0m \n",
      "9_300_0.010_07_0.85 \u001b[34m0.767466\u001b[0m \n",
      "9_300_0.010_08_0.75 \u001b[34m0.768065\u001b[0m \n",
      "9_300_0.010_08_0.80 \u001b[34m0.767771\u001b[0m \n",
      "9_300_0.010_08_0.85 \u001b[34m0.766981\u001b[0m \n",
      "9_300_0.010_09_0.75 \u001b[34m0.768274\u001b[0m \n",
      "9_300_0.010_09_0.80 \u001b[34m0.767626\u001b[0m \n",
      "9_300_0.010_09_0.85 \u001b[34m0.766894\u001b[0m \n",
      "9_300_0.010_10_0.75 \u001b[34m0.767883\u001b[0m \n",
      "9_300_0.010_10_0.80 \u001b[34m0.767619\u001b[0m \n",
      "9_300_0.010_10_0.85 \u001b[34m0.767033\u001b[0m \n",
      "------------------------------\n",
      "{'max_depth': 8, 'n_estimators': 300, 'learning_rate': 0.01, 'min_child_weight': 9, 'colsample_bytree': 0.75, 'verbosity': 0, 'objective': 'binary:logistic', 'booster': 'gbtree', 'subsample': 0.8} = \u001b[34m0.769046\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "auc, para = grid_search(max_depth_arr=[8,9],n_estimators_arr=[200,250,300],learning_rate_arr=[0.010], min_child_arr=range(1,11), col_sample_arr=[0.75,0.8,0.85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_lgbm(max_depth_arr, leaves_arr, n_estimators_arr, learning_rate_arr, weight_arr, col_sample_arr):\n",
    "    opt_auc = 0\n",
    "    opt_para = {}\n",
    "   \n",
    "    train_x = preprocess(train,'train')\n",
    "    train_y = train_x['voted']\n",
    "    train_x = train_x.drop(['voted'],axis=1)\n",
    "    val_x = preprocess(val,'test')\n",
    "    val_y = val_x['voted']\n",
    "    val_x = val_x.drop(['voted'],axis=1)\n",
    "    \n",
    "    for max_depth in max_depth_arr:\n",
    "        for num_leaves in leaves_arr:\n",
    "            for n_estimators in n_estimators_arr:\n",
    "                for learning_rate in learning_rate_arr:\n",
    "                    #for samples in samples_arr:\n",
    "                    for weight in weight_arr:\n",
    "                        for col_sample in col_sample_arr:\n",
    "                            print(f'{num_leaves}_{n_estimators}_{learning_rate:.3f}_{weight:03d}_{col_sample:.2f}', end=' ')\n",
    "                            param = { 'num_leaves' : num_leaves,\n",
    "                                    'max_depth' : max_depth,\n",
    "                                    'n_estimators' : n_estimators,\n",
    "                                    'learning_rate' : learning_rate,\n",
    "                                    #'min_child_samples' : samples,\n",
    "                                    'min_child_weight' : weight,\n",
    "                                    'colsample_bytree' : col_sample,\n",
    "                                    'verbosity' : -1,\n",
    "                                    'objective' : 'binary',\n",
    "                                    'boosting_type' : 'dart',\n",
    "                                    'subsample' : 0.8,\n",
    "                                    'max_depth' : -1,\n",
    "                                    'force_row_wise' : True}\n",
    "\n",
    "                            model = LGBMClassifier(**param)\n",
    "                            model.fit(train_x, train_y, verbose=False)\n",
    "                            auc = train_auc([model], val_x, val_y) \n",
    "                            print('\\033[34m' + f'{auc:.6f}' + '\\033[0m', end=' ')\n",
    "\n",
    "                            if (auc>opt_auc):\n",
    "                                opt_auc = auc\n",
    "                                opt_para = param\n",
    "                                print('\\033[31m' + f'{num_leaves}_{n_estimators}_{learning_rate:.3f}_{weight:03d}_{col_sample:.2f}' + '\\033[0m')\n",
    "                            else:\n",
    "                                print()\n",
    "    \n",
    "    print('-'*30)\n",
    "    print(f'{opt_para} = ' + '\\033[34m' + f'{opt_auc:.6f}' + '\\033[0m')\n",
    "    \n",
    "    return opt_auc, opt_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300_300_0.010_050_0.80 \u001b[34m0.767353\u001b[0m \u001b[31m300_300_0.010_050_0.80\u001b[0m\n",
      "------------------------------\n",
      "{'num_leaves': 300, 'max_depth': -1, 'n_estimators': 300, 'learning_rate': 0.01, 'min_child_weight': 50, 'colsample_bytree': 0.8, 'verbosity': -1, 'objective': 'binary', 'boosting_type': 'dart', 'subsample': 0.8, 'force_row_wise': True} = \u001b[34m0.767353\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "auc, para = grid_search_lgbm(max_depth_arr = [7], leaves_arr=[300], n_estimators_arr=[300], learning_rate_arr=[0.01], weight_arr=[50], col_sample_arr=[0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
